# TODO: add papers by configuration file
base_url: "https://arxiv.paperswithcode.com/api/v0/papers/"
user_name: "Angel0703over"
repo_name: "llm-arxiv-daily"
show_authors: True
show_links: True
show_badge: True
max_results: 10

publish_readme: True
publish_gitpage: True
publish_wechat: False

# file paths
json_readme_path: './docs/cv-arxiv-daily.json'
json_gitpage_path: './docs/cv-arxiv-daily-web.json'
json_wechat_path: './docs/cv-arxiv-daily-wechat.json'

md_readme_path: 'README.md'
md_gitpage_path: './docs/index.md'
md_wechat_path: './docs/wechat.md'

# keywords to search
keywords:
    # ===== 训练类 =====
    "Large Language Models":
        filters: ["Large Language Model", "LLM", "Language Model", "GPT", "ChatGPT", "LLaMA", "Mistral"]
    "Instruction Tuning":
        filters: ["Instruction Tuning", "Instruction Following", "Alignment", "Supervised Fine-tuning"]
    "Reinforcement Learning with Human Feedback":
        filters: ["RLHF", "Reinforcement Learning with Human Feedback", "Preference Optimization"]
    "Efficient LLM":
        filters: ["Model Compression", "Quantization", "Pruning", "LoRA", "PEFT", "Parameter-Efficient Fine-tuning"]

    # ===== 推理类 =====
    "LLM Inference Service":
        filters: ["LLM Inference", "Model Serving", "Distributed Inference",
                  "Inference Acceleration", "Continuous Batching",
                  "Triton Inference Server", "vLLM", "DeepSpeed-MII", "FlashAttention"]

    # ===== 应用类 =====
    "Retrieval-Augmented Generation":
        filters: ["RAG", "Retrieval-Augmented Generation", "Knowledge-Augmented Generation"]
    "Multimodal LLM":
        filters: ["Multimodal LLM", "Vision Language Model", "VLM", "LVLM", "Multimodal Generation"]
    "Evaluation":
        filters: ["LLM Evaluation", "Benchmarking", "Hallucination", "Bias", "Robustness"]
    "Applications":
        filters: ["Code Generation", "Reasoning", "Mathematical Reasoning", "Tool Use", "Agent"]

